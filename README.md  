1. app.py (Main API - Handles Queries & File Uploads)
python
Copy
Edit
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
import uvicorn
import ollama
from retrieval import retrieve_relevant_text, process_uploaded_file
from storage import store_text_in_chroma

app = FastAPI()

# Define educational topics
EDUCATIONAL_TOPICS = {
    "math", "science", "history", "programming", "literature",
    "technology", "engineering", "education", "learning", "physics",
    "chemistry", "biology", "coding", "artificial intelligence"
}

def is_educational_query(query: str) -> bool:
    """Checks if the query is educational."""
    return any(topic in query.lower() for topic in EDUCATIONAL_TOPICS)

class QueryRequest(BaseModel):
    question: str

@app.post("/ask")
async def answer_question(request: QueryRequest):
    """Handles user queries, filtering non-educational ones."""
    if not is_educational_query(request.question):
        return {"answer": "I'm here to assist with educational topics. Please ask about math, science, programming, or related subjects."}
    
    try:
        # Retrieve relevant text from storage, Wikipedia, or scraping
        context = retrieve_relevant_text(request.question)

        if not context or "No relevant data found" in context:
            response = ollama.chat(model="llama3", messages=[
                {"role": "system", "content": "You are an AI tutor that provides responses only related to education. If a user asks something irrelevant, politely ask them to focus on educational topics."},
                {"role": "user", "content": request.question}
            ])
            return {"answer": response.get('message', {}).get('content', "I couldn't find a relevant answer.")}
        
        return {"answer": context}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """Handles file uploads for study materials (PDF, DOCX, TXT)."""
    try:
        text = process_uploaded_file(file)
        store_text_in_chroma(text, file.filename)
        return {"message": "File processed and stored successfully", "extracted_text": text}
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
2. retrieval.py (Retrieving & Scraping Data)
python
Copy
Edit
import chromadb
from sentence_transformers import SentenceTransformer
import wikipedia
import requests
from bs4 import BeautifulSoup
import PyPDF2
import docx
from storage import get_chroma_collection

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Educational websites for scraping
EDUCATIONAL_SITES = [
    "https://www.khanacademy.org/",
    "https://www.coursera.org/",
    "https://www.edx.org/"
]

def scrape_educational_websites(query):
    """Scrapes educational websites for study material."""
    for site in EDUCATIONAL_SITES:
        try:
            response = requests.get(site, timeout=5)
            soup = BeautifulSoup(response.text, "html.parser")
            text = " ".join(p.text for p in soup.find_all("p"))
            if query.lower() in text.lower():
                return f"From {site}: {text[:300]}..."
        except requests.RequestException:
            continue
    return None

def retrieve_relevant_text(query):
    """Retrieves educational content from ChromaDB, Wikipedia, or Web Scraping."""
    collection = get_chroma_collection()
    query_embedding = model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=3)

    if results and results["documents"]:
        return " ".join(results["documents"])

    # Try Wikipedia search
    try:
        wiki_summary = wikipedia.summary(query, sentences=2)
        return f"Wikipedia says: {wiki_summary}"
    except wikipedia.exceptions.PageError:
        pass
    except wikipedia.exceptions.DisambiguationError:
        pass

    # Web scraping as a last resort
    web_result = scrape_educational_websites(query)
    if web_result:
        return web_result

    return "No relevant data found. Try rephrasing your question."

def extract_text_from_pdf(file):
    """Extracts text from a PDF file."""
    reader = PyPDF2.PdfReader(file)
    text = "".join([page.extract_text() for page in reader.pages if page.extract_text()])
    return text

def extract_text_from_docx(file):
    """Extracts text from a DOCX file."""
    doc = docx.Document(file)
    return "\n".join([para.text for para in doc.paragraphs])

def process_uploaded_file(file):
    """Processes uploaded PDF, DOCX, or TXT files."""
    if file.filename.endswith(".pdf"):
        return extract_text_from_pdf(file.file)
    elif file.filename.endswith(".docx"):
        return extract_text_from_docx(file.file)
    elif file.filename.endswith(".txt"):
        return file.file.read().decode("utf-8")
    else:
        raise ValueError("Unsupported file format. Please upload PDF, DOCX, or TXT.")
3. storage.py (ChromaDB Data Storage)
python
Copy
Edit
import chromadb
from sentence_transformers import SentenceTransformer

# Initialize ChromaDB client
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection("study_material")

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

def get_chroma_collection():
    """Returns the ChromaDB collection instance."""
    return collection

def store_text_in_chroma(text, source):
    """Stores extracted text in ChromaDB with embeddings."""
    embedding = model.encode(text).tolist()
    collection.add(
        documents=[text],
        embeddings=[embedding],
        metadatas=[{"source": source}]
    )
4. requirements.txt (Install Dependencies)
nginx
Copy
Edit
fastapi
uvicorn
chromadb
sentence-transformers
ollama
wikipedia-api
pydantic
beautifulsoup4
requests
PyPDF2
python-docx
Project Structure
bash
Copy
Edit
/ai-study-helper-chatbot
│── app.py                # FastAPI backend
│── retrieval.py          # Retrieval, scraping, and file processing
│── storage.py            # ChromaDB storage
│── requirements.txt      # Required Python packages
│── chroma_db/            # Persistent storage for embeddings
Key Features
✅ API for user queries & file uploads
✅ Retrieves data from ChromaDB, Wikipedia, and educational websites
✅ Extracts and stores data from PDFs, DOCX, and TXT files
✅ Uses sentence-transformers to create embeddings and ChromaDB for fast retrieval
✅ Clean separation of concerns: API (app.py), Retrieval (retrieval.py), Storage (storage.py)